{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add extension to find exectution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.41 s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import log\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.92 ms\n"
     ]
    }
   ],
   "source": [
    "documents = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "labels = [0, 1, 0, 1, 0, 1]\n",
    "#1 is abusive, 0 not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 134 ms\n"
     ]
    }
   ],
   "source": [
    "def create_vocab(documents):\n",
    "    \"\"\"\n",
    "    Create set of all words that are present in all documents\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    for document in documents:\n",
    "        vocab |= set(document)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary in the documents : {'so', 'dog', 'garbage', 'stop', 'mr', 'dalmation', 'not', 'posting', 'cute', 'steak', 'licks', 'how', 'park', 'stupid', 'has', 'him', 'worthless', 'quit', 'I', 'to', 'buying', 'my', 'maybe', 'ate', 'help', 'take', 'love', 'problems', 'please', 'food', 'flea', 'is'}\n",
      "time: 102 ms\n"
     ]
    }
   ],
   "source": [
    "vocab = create_vocab(documents)\n",
    "print(f\"Vocabulary in the documents : {create_vocab(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating vector from a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 84.7 ms\n"
     ]
    }
   ],
   "source": [
    "def word2vec(document, vocab):\n",
    "    word_vector = [0] * len(vocab)\n",
    "    vocab_list = list(vocab)\n",
    "    for token in document:\n",
    "        if token in vocab:\n",
    "            word_vector[vocab_list.index(token)] = 1\n",
    "    return word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector for first document is : \n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n",
      "Vocabulary being : \n",
      "{'so', 'dog', 'garbage', 'stop', 'mr', 'dalmation', 'not', 'posting', 'cute', 'steak', 'licks', 'how', 'park', 'stupid', 'has', 'him', 'worthless', 'quit', 'I', 'to', 'buying', 'my', 'maybe', 'ate', 'help', 'take', 'love', 'problems', 'please', 'food', 'flea', 'is'}\n",
      "Document being : \n",
      "['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']\n",
      "Looks good!\n",
      "time: 248 ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"Word vector for first document is : \\n{word2vec(documents[0], vocab)}\")\n",
    "print(f\"Vocabulary being : \\n{vocab}\")\n",
    "print(f\"Document being : \\n{documents[0]}\")\n",
    "print(\"Looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 84.4 ms\n"
     ]
    }
   ],
   "source": [
    "document_vectors = [word2vec(document, vocab) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 documents\n",
      "There are 32 features\n",
      "time: 52.8 ms\n"
     ]
    }
   ],
   "source": [
    "len_train_docs = len(document_vectors)\n",
    "len_words_vocab = len(vocab)\n",
    "print(f\"There are {len_train_docs} documents\")\n",
    "print(f\"There are {len_words_vocab} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of a document being abusive is 0.5\n",
      "time: 1.81 ms\n"
     ]
    }
   ],
   "source": [
    "p_abusive = sum(labels) / len_train_docs\n",
    "print(f\"Probability of a document being abusive is {p_abusive}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 703 µs\n"
     ]
    }
   ],
   "source": [
    "p0_num, p1_num = np.zeros(len_words_vocab), np.zeros(len_words_vocab)\n",
    "p0_denom, p1_denom = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.48 ms\n"
     ]
    }
   ],
   "source": [
    "for index in range(len_train_docs):\n",
    "    if labels[index] == 1: # If the document belongs to class 1\n",
    "        p1_num += document_vectors[index] # Add occurances\n",
    "        p1_denom += sum(document_vectors[index]) # Add sum of occurances\n",
    "    \n",
    "    else:\n",
    "        p0_num += document_vectors[index]\n",
    "        p0_denom += sum(document_vectors[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condititonal probability vector for class 1 : \n",
      " [0.         0.10526316 0.05263158 0.05263158 0.         0.\n",
      " 0.05263158 0.05263158 0.         0.         0.         0.\n",
      " 0.05263158 0.15789474 0.         0.05263158 0.10526316 0.05263158\n",
      " 0.         0.05263158 0.05263158 0.         0.05263158 0.\n",
      " 0.         0.05263158 0.         0.         0.         0.05263158\n",
      " 0.         0.        ]\n",
      "Condititonal probability vector for class 0 : \n",
      " [0.04166667 0.04166667 0.         0.04166667 0.04166667 0.04166667\n",
      " 0.         0.         0.04166667 0.04166667 0.04166667 0.04166667\n",
      " 0.         0.         0.04166667 0.08333333 0.         0.\n",
      " 0.04166667 0.04166667 0.         0.125      0.         0.04166667\n",
      " 0.04166667 0.         0.04166667 0.04166667 0.04166667 0.\n",
      " 0.04166667 0.04166667]\n",
      "Conditional probability here is : Probability of occurance of the word, if it belongs to the given class\n",
      "time: 3.7 ms\n"
     ]
    }
   ],
   "source": [
    "p1_vect = p1_num / p1_denom\n",
    "p0_vect = p0_num / p0_denom\n",
    "print(f\"Condititonal probability vector for class 1 : \\n {p1_vect}\")\n",
    "print(f\"Condititonal probability vector for class 0 : \\n {p0_vect}\")\n",
    "print(\"Conditional probability here is : Probability of occurance of the word, if it belongs to the given class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we attempt to classify a document, we multiply a lot of probabilities together to\n",
    "get the probability that a document belongs to a given class. This will look something\n",
    "like p(w 0 |1)p(w 1 |1)p(w 2 |1) . If any of these numbers are 0, then when we multiply\n",
    "them together we get 0. To lessen the impact of this, we’ll initialize all of our occur-\n",
    "rence counts to 1, and we’ll initialize the denominators to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.51 ms\n"
     ]
    }
   ],
   "source": [
    "p0_num, p1_num = np.ones(len_words_vocab), np.ones(len_words_vocab)\n",
    "p0_denom, p1_denom = 2.0, 2.0\n",
    "\n",
    "for index in range(len_train_docs):\n",
    "    if labels[index] == 1: # If the document belongs to class 1\n",
    "        p1_num += document_vectors[index] # Add occurances\n",
    "        p1_denom += sum(document_vectors[index]) # Add sum of occurances\n",
    "    \n",
    "    else:\n",
    "        p0_num += document_vectors[index]\n",
    "        p0_denom += sum(document_vectors[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another problem is underflow: doing too many multiplications of small numbers.\n",
    "When we go to calculate the product p(w 0 |c i )p(w 1 |c i )p(w 2 |c i )...p(w N |c i ) and many\n",
    "of these numbers are very small, we’ll get underflow, or an incorrect answer. We use natural logarithm here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 986 µs\n"
     ]
    }
   ],
   "source": [
    "p1_vect = np.log(p1_num / p1_denom)\n",
    "p0_vect = np.log(p0_num / p0_denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.04452244, -1.94591015, -2.35137526, -2.35137526, -3.04452244,\n",
       "       -3.04452244, -2.35137526, -2.35137526, -3.04452244, -3.04452244,\n",
       "       -3.04452244, -3.04452244, -2.35137526, -1.65822808, -3.04452244,\n",
       "       -2.35137526, -1.94591015, -2.35137526, -3.04452244, -2.35137526,\n",
       "       -2.35137526, -3.04452244, -2.35137526, -3.04452244, -3.04452244,\n",
       "       -2.35137526, -3.04452244, -3.04452244, -3.04452244, -2.35137526,\n",
       "       -3.04452244, -3.04452244])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([-2.56494936, -2.56494936, -3.25809654, -2.56494936, -2.56494936,\n",
       "       -2.56494936, -3.25809654, -3.25809654, -2.56494936, -2.56494936,\n",
       "       -2.56494936, -2.56494936, -3.25809654, -3.25809654, -2.56494936,\n",
       "       -2.15948425, -3.25809654, -3.25809654, -2.56494936, -2.56494936,\n",
       "       -3.25809654, -1.87180218, -3.25809654, -2.56494936, -2.56494936,\n",
       "       -3.25809654, -2.56494936, -2.56494936, -2.56494936, -3.25809654,\n",
       "       -2.56494936, -2.56494936])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.02 ms\n"
     ]
    }
   ],
   "source": [
    "p1_vect\n",
    "p0_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify the new vector based on the conditional probability and probability of it belonging to one class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p(ci / w) = p(w / ci) * p(ci) / p(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.91 ms\n"
     ]
    }
   ],
   "source": [
    "def classifier(vector, p0_vect, p1_vect, p_class1):\n",
    "    p_class0 = 1 - p_class1\n",
    "    p1 = np.sum(vector * p1_vect) / np.log(p_class1)\n",
    "    p0 = np.sum(vector * p0_vect) / np.log(p_class0)\n",
    "    return 0 if p1 > p0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] is classified as 0\n",
      "time: 1.48 ms\n"
     ]
    }
   ],
   "source": [
    "document = ['love', 'my', 'dalmation']\n",
    "test_vector = np.array(word2vec(document, vocab))\n",
    "print(f\"{document} is classified as {classifier(test_vector, p0_vect, p1_vect, p_abusive)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stupid', 'garbage'] is classified as 1\n",
      "time: 1.29 ms\n"
     ]
    }
   ],
   "source": [
    "document = ['stupid', 'garbage']\n",
    "test_vector = np.array(word2vec(document, vocab))\n",
    "print(f\"{document} is classified as {classifier(test_vector, p0_vect, p1_vect, p_abusive)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until this point we’ve treated the presence or absence of a word as a feature. This\n",
    "could be described as a set-of-words model. If a word appears more than once in a\n",
    "document, that might convey some sort of information about the document over just\n",
    "the word occurring in the document or not. This approach is known as a bag-of-words\n",
    "model. A bag of words can have multiple occurrences of each word, whereas a set of\n",
    "words can have only one occurrence of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 987 µs\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words_vec(document, vocab):\n",
    "    word_vector = [0] * len(vocab)\n",
    "    vocab_list = list(vocab)\n",
    "    for token in document:\n",
    "        if token in vocab:\n",
    "            word_vector[vocab_list.index(token)] += 1\n",
    "    return word_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuations and split sentences to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.39 ms\n"
     ]
    }
   ],
   "source": [
    "def clean_string(text_str):\n",
    "    list_tokens = re.split(r'\\W*', text_str)\n",
    "    return [tok.lower() for tok in list_tokens if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.9 ms\n"
     ]
    }
   ],
   "source": [
    "docs = [Path(\"../data/email/spam\"), Path(\"../data/email/ham\")]\n",
    "labels = list()\n",
    "doc_list, full_text = list(), list()\n",
    "for path_docs in docs:\n",
    "    for text_file in path_docs.glob(\"*.txt\"):\n",
    "        with open(text_file, \"r\", encoding=\"utf8\", errors='ignore') as f:\n",
    "            text = f.read()\n",
    "        clean_text = clean_string(text)\n",
    "        doc_list.append(clean_text)\n",
    "        full_text.extend(clean_text)\n",
    "        labels.append(1 if \"spam\" in str(text_file) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 998 µs\n"
     ]
    }
   ],
   "source": [
    "vocab = create_vocab(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.02 ms\n"
     ]
    }
   ],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(doc_list, labels, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16.5 ms\n"
     ]
    }
   ],
   "source": [
    "train_document_vectors = [bag_of_words_vec(document, vocab) for document in features_train]\n",
    "test_document_vectors = [bag_of_words_vec(document, vocab) for document in features_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.56 ms\n"
     ]
    }
   ],
   "source": [
    "def train_nb(document_vectors, labels):\n",
    "    len_train_docs = len(document_vectors)\n",
    "    len_words_vocab = len(vocab)\n",
    "    p_abusive = sum(labels) / len_train_docs\n",
    "\n",
    "    p0_num, p1_num = np.ones(len_words_vocab), np.ones(len_words_vocab)\n",
    "    p0_denom, p1_denom = 2.0, 2.0\n",
    "\n",
    "    for index in range(len_train_docs):\n",
    "        if labels[index] == 1: # If the document belongs to class 1\n",
    "            p1_num += document_vectors[index] # Add occurances\n",
    "            p1_denom += sum(document_vectors[index]) # Add sum of occurances\n",
    "\n",
    "        else:\n",
    "            p0_num += document_vectors[index]\n",
    "            p0_denom += sum(document_vectors[index])\n",
    "\n",
    "    p1_vect = np.log(p1_num / p1_denom)\n",
    "    p0_vect = np.log(p0_num / p0_denom)\n",
    "    return p1_vect, p0_vect, p_abusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.73 ms\n"
     ]
    }
   ],
   "source": [
    "p1_vect, p0_vect, p_spam = train_nb(train_document_vectors, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error count is 0.0\n",
      "time: 4.06 ms\n"
     ]
    }
   ],
   "source": [
    "error_count = 0\n",
    "for ind, doc_vector in enumerate(test_document_vectors):\n",
    "    if classifier(doc_vector, p0_vect, p1_vect, p_spam) != labels_test[ind]:\n",
    "        error_count += 1\n",
    "print(f\"Error count is {error_count / len(test_document_vectors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset is small, we are able to get 100 percent accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
