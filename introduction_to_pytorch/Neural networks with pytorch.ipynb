{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transform to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.5,), std=(0.5,))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.MNIST(\"~/.pytorch/MNIST_data/\", download=True, train=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: /home/amog/.pytorch/MNIST_data/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f5ce8735400>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The batch size is the number of images we get in one iteration from the data loader and pass through our network, often called a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "images.shape\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that images is just a tensor with size (64, 1, 28, 28). So, 64 images per batch, 1 color channel, and 28x28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5cdcc52128>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[2].numpy().squeeze(), cmap='Greys_r')\n",
    "labels[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.view(-1, 784).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Activation function\n",
    "    \"\"\"\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_units = 784\n",
    "hidden_units = 256\n",
    "output_units = 10\n",
    "\n",
    "batch_size = images.shape[0]\n",
    "images = images.view((-1, input_units))                # Flatten the images\n",
    "\n",
    "w1 = torch.randn((input_units, hidden_units))          # Hidden layer\n",
    "w2 = torch.randn((hidden_units, output_units))         # Output layer\n",
    "\n",
    "b1 = torch.randn((1, hidden_units))                    # Hidden layer bias\n",
    "b2 = torch.randn((1, output_units))                    # Output layer bias\n",
    "\n",
    "h1 = sigmoid(torch.mm(images, w1) + b1)                # activation(wx + b) on hidden layer\n",
    "h2 = sigmoid(torch.mm(h1, w2) + b2)                    # activation(wx + b) on output layer\n",
    "\n",
    "h2.shape # for each image, we have 10 outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to have outputs per image to be in 10 classes whose sum should add up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.exp(-h2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.5767, 1.0000, 0.9981, 0.9651, 0.3913, 0.9943, 1.0000, 0.3679,\n",
       "        0.9834])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.2767)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1208, 0.0697, 0.1208, 0.1206, 0.1166, 0.0473, 0.1201, 0.1208, 0.0445,\n",
       "        0.1188])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a / torch.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(a / torch.sum(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the sum is 1. Now we have to do it on whole tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_tensor = torch.exp(-h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.5767, 1.0000, 0.9981, 0.9651, 0.3913, 0.9943, 1.0000, 0.3679,\n",
       "         0.9834],\n",
       "        [1.0000, 0.9899, 0.9911, 0.9984, 0.7812, 0.3684, 0.5936, 1.0000, 0.9999,\n",
       "         0.8942],\n",
       "        [1.0000, 0.7817, 0.8755, 1.0000, 0.3679, 0.3679, 0.9992, 1.0000, 0.4527,\n",
       "         0.9608],\n",
       "        [1.0000, 0.3683, 0.9806, 0.9999, 0.3727, 0.3679, 0.9824, 1.0000, 0.9999,\n",
       "         0.3979],\n",
       "        [1.0000, 0.3705, 0.9995, 0.9997, 0.3695, 0.3679, 0.3707, 1.0000, 0.5539,\n",
       "         0.6568],\n",
       "        [1.0000, 0.3679, 0.9994, 1.0000, 0.4626, 0.3679, 0.3679, 1.0000, 0.3680,\n",
       "         0.3681],\n",
       "        [1.0000, 0.9658, 0.9961, 0.7221, 0.3679, 0.3679, 0.3679, 1.0000, 0.3691,\n",
       "         0.9930],\n",
       "        [1.0000, 0.3681, 0.8292, 1.0000, 0.3710, 0.3681, 0.4347, 1.0000, 0.3720,\n",
       "         0.3716],\n",
       "        [1.0000, 0.9493, 1.0000, 1.0000, 0.3680, 0.9886, 0.9998, 1.0000, 0.6244,\n",
       "         0.9995],\n",
       "        [1.0000, 0.8287, 0.7316, 0.4720, 0.3679, 0.3679, 0.6908, 1.0000, 0.4775,\n",
       "         0.9076],\n",
       "        [1.0000, 0.3733, 0.9926, 0.5425, 0.5110, 0.3681, 0.3730, 1.0000, 0.7054,\n",
       "         0.5636],\n",
       "        [1.0000, 0.3681, 0.9967, 0.9449, 0.9730, 0.3679, 0.9301, 1.0000, 0.9403,\n",
       "         0.9998],\n",
       "        [1.0000, 0.3693, 0.9878, 0.9944, 0.3684, 0.3679, 0.4156, 1.0000, 0.9947,\n",
       "         0.7151],\n",
       "        [1.0000, 0.3680, 1.0000, 0.9993, 0.3694, 0.6349, 0.8723, 1.0000, 0.5931,\n",
       "         0.9998],\n",
       "        [1.0000, 0.3679, 1.0000, 1.0000, 0.3681, 0.3679, 0.9117, 1.0000, 0.8892,\n",
       "         1.0000],\n",
       "        [1.0000, 0.3742, 0.5028, 0.9998, 0.3687, 0.3679, 0.4455, 1.0000, 0.3753,\n",
       "         0.8895],\n",
       "        [1.0000, 0.3686, 0.9927, 0.7312, 0.3690, 0.9569, 0.3682, 1.0000, 0.3709,\n",
       "         0.4631],\n",
       "        [1.0000, 0.3877, 1.0000, 0.9925, 0.9727, 0.3746, 0.4995, 1.0000, 1.0000,\n",
       "         0.3787],\n",
       "        [0.8765, 0.5446, 0.9864, 1.0000, 0.3679, 0.3679, 0.3680, 1.0000, 0.8953,\n",
       "         0.6575],\n",
       "        [0.9976, 0.7241, 0.8862, 1.0000, 0.3679, 0.3679, 0.3688, 1.0000, 0.9999,\n",
       "         0.4471],\n",
       "        [1.0000, 0.9125, 0.9996, 1.0000, 0.3679, 0.3679, 0.3937, 1.0000, 0.4242,\n",
       "         0.9437],\n",
       "        [1.0000, 0.3679, 0.9998, 0.6357, 0.3690, 0.3686, 0.3679, 1.0000, 0.3974,\n",
       "         0.8289],\n",
       "        [0.9996, 0.3680, 1.0000, 1.0000, 0.3883, 0.4215, 0.4574, 1.0000, 0.5449,\n",
       "         0.3681],\n",
       "        [0.8520, 0.6214, 1.0000, 0.9999, 0.7139, 0.3680, 0.3679, 1.0000, 0.3700,\n",
       "         0.7989],\n",
       "        [1.0000, 0.3679, 0.6348, 0.3851, 0.4585, 0.3679, 0.3714, 1.0000, 0.8082,\n",
       "         0.3684],\n",
       "        [1.0000, 0.3726, 0.9971, 0.8269, 0.3684, 0.3679, 0.9927, 1.0000, 0.9998,\n",
       "         0.7605],\n",
       "        [0.9978, 0.9942, 1.0000, 1.0000, 0.3679, 0.3679, 0.3680, 1.0000, 0.9995,\n",
       "         0.3835],\n",
       "        [1.0000, 0.3682, 0.5700, 0.9964, 0.4287, 0.3679, 0.3690, 1.0000, 0.3866,\n",
       "         0.9906],\n",
       "        [1.0000, 0.3679, 1.0000, 1.0000, 0.3683, 0.3679, 0.3797, 1.0000, 0.4172,\n",
       "         0.3679],\n",
       "        [1.0000, 0.3679, 1.0000, 0.9973, 0.3713, 0.3698, 0.4929, 1.0000, 0.3681,\n",
       "         0.3696],\n",
       "        [1.0000, 0.3679, 0.9659, 0.9961, 0.4596, 0.3681, 0.4675, 1.0000, 0.3935,\n",
       "         0.4967],\n",
       "        [1.0000, 0.5092, 0.9973, 1.0000, 0.4590, 0.3762, 0.3679, 1.0000, 0.4989,\n",
       "         0.3695],\n",
       "        [1.0000, 0.8999, 1.0000, 0.9998, 0.6144, 0.3685, 0.4717, 1.0000, 0.6232,\n",
       "         0.9255],\n",
       "        [1.0000, 0.3714, 0.9899, 0.9975, 0.3679, 0.3751, 0.3698, 1.0000, 0.9985,\n",
       "         0.6843],\n",
       "        [1.0000, 0.9162, 0.9965, 0.9971, 0.3681, 0.3679, 0.9648, 1.0000, 0.4033,\n",
       "         1.0000],\n",
       "        [1.0000, 0.3689, 0.9999, 0.9752, 0.3685, 0.3680, 0.3683, 1.0000, 0.7679,\n",
       "         0.3694],\n",
       "        [1.0000, 0.3690, 0.9998, 0.9999, 0.4236, 0.3756, 0.3680, 1.0000, 0.3707,\n",
       "         0.4905],\n",
       "        [0.9982, 0.9160, 1.0000, 0.9998, 0.3975, 0.3679, 0.4460, 1.0000, 0.6603,\n",
       "         0.5018],\n",
       "        [1.0000, 0.4619, 0.9975, 0.9999, 0.5493, 0.3679, 0.9604, 1.0000, 0.9969,\n",
       "         1.0000],\n",
       "        [1.0000, 0.3680, 0.4316, 0.3785, 0.3792, 0.3683, 0.3729, 1.0000, 0.3679,\n",
       "         0.4622],\n",
       "        [1.0000, 0.3750, 0.9875, 0.9687, 0.3680, 0.3679, 0.9546, 1.0000, 0.4250,\n",
       "         0.4608],\n",
       "        [1.0000, 0.3807, 1.0000, 1.0000, 0.3770, 0.3680, 0.9875, 1.0000, 0.5530,\n",
       "         0.3797],\n",
       "        [1.0000, 0.3912, 0.9995, 0.9963, 0.8731, 0.3679, 0.9864, 1.0000, 0.8423,\n",
       "         0.9766],\n",
       "        [1.0000, 0.5279, 0.9988, 1.0000, 0.3683, 0.3992, 0.7917, 1.0000, 0.9998,\n",
       "         0.6457],\n",
       "        [1.0000, 0.3686, 1.0000, 0.3851, 0.3693, 0.3679, 0.3753, 1.0000, 0.9816,\n",
       "         0.9789],\n",
       "        [1.0000, 0.7875, 1.0000, 1.0000, 0.3689, 0.5131, 0.4362, 1.0000, 0.3679,\n",
       "         0.7297],\n",
       "        [1.0000, 0.3680, 1.0000, 1.0000, 0.4004, 0.9059, 0.3685, 1.0000, 0.3679,\n",
       "         0.3877],\n",
       "        [0.9990, 0.7246, 0.9995, 0.9995, 0.3679, 0.3679, 0.3683, 1.0000, 0.9997,\n",
       "         0.4914],\n",
       "        [0.9998, 0.7078, 0.7362, 0.9581, 0.5555, 0.3680, 0.4174, 1.0000, 0.3695,\n",
       "         0.4386],\n",
       "        [1.0000, 0.3782, 0.9979, 0.9988, 0.3680, 0.3679, 0.9998, 1.0000, 0.3750,\n",
       "         0.7326],\n",
       "        [1.0000, 0.9988, 0.9986, 1.0000, 0.3679, 0.3687, 0.4018, 1.0000, 0.9013,\n",
       "         0.8473],\n",
       "        [1.0000, 0.3699, 0.5269, 1.0000, 0.3697, 0.3679, 0.4399, 1.0000, 0.3681,\n",
       "         0.9914],\n",
       "        [1.0000, 0.3718, 0.9949, 0.9999, 0.4988, 0.3764, 0.9830, 1.0000, 0.9447,\n",
       "         0.9960],\n",
       "        [1.0000, 0.9957, 1.0000, 1.0000, 0.3830, 0.3681, 0.3835, 1.0000, 1.0000,\n",
       "         0.3805],\n",
       "        [1.0000, 0.3740, 0.6281, 0.6174, 0.4229, 0.3769, 1.0000, 1.0000, 0.3726,\n",
       "         1.0000],\n",
       "        [1.0000, 0.3799, 1.0000, 1.0000, 0.3679, 0.3695, 0.8999, 1.0000, 0.3679,\n",
       "         0.4918],\n",
       "        [1.0000, 0.3708, 0.9993, 1.0000, 0.3711, 0.3680, 0.3728, 1.0000, 0.8200,\n",
       "         0.3828],\n",
       "        [1.0000, 0.3679, 0.9999, 1.0000, 0.9979, 0.3710, 0.9526, 1.0000, 0.9352,\n",
       "         0.4079],\n",
       "        [1.0000, 0.4161, 1.0000, 0.9999, 0.3697, 0.3679, 0.8456, 1.0000, 0.7964,\n",
       "         0.6344],\n",
       "        [0.9999, 0.3804, 1.0000, 1.0000, 0.3700, 0.3738, 0.3679, 1.0000, 0.8747,\n",
       "         0.7002],\n",
       "        [1.0000, 0.9644, 0.9133, 0.9987, 0.3679, 0.3679, 1.0000, 1.0000, 0.9458,\n",
       "         0.9577],\n",
       "        [1.0000, 0.3679, 1.0000, 1.0000, 0.8400, 0.3695, 0.8081, 1.0000, 0.3792,\n",
       "         0.9986],\n",
       "        [1.0000, 0.4779, 0.7434, 1.0000, 0.3686, 0.3698, 0.4765, 1.0000, 0.7100,\n",
       "         0.9997],\n",
       "        [1.0000, 0.9719, 1.0000, 0.9791, 0.3776, 0.3680, 0.7434, 1.0000, 0.9316,\n",
       "         0.4020]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.2767],\n",
       "        [8.6166],\n",
       "        [7.8058],\n",
       "        [7.4696],\n",
       "        [6.6885],\n",
       "        [6.3018],\n",
       "        [7.1498],\n",
       "        [6.1147],\n",
       "        [8.9298],\n",
       "        [6.8440],\n",
       "        [6.4295],\n",
       "        [8.5209],\n",
       "        [7.2133],\n",
       "        [7.8367],\n",
       "        [7.9047],\n",
       "        [6.3237],\n",
       "        [6.6206],\n",
       "        [7.6057],\n",
       "        [7.0640],\n",
       "        [7.1595],\n",
       "        [7.4094],\n",
       "        [6.3351],\n",
       "        [6.5478],\n",
       "        [7.0921],\n",
       "        [5.7621],\n",
       "        [7.6857],\n",
       "        [7.4787],\n",
       "        [6.4775],\n",
       "        [6.2688],\n",
       "        [6.3368],\n",
       "        [6.5152],\n",
       "        [6.5778],\n",
       "        [7.9030],\n",
       "        [7.1541],\n",
       "        [8.0138],\n",
       "        [6.5860],\n",
       "        [6.3970],\n",
       "        [7.2875],\n",
       "        [8.3338],\n",
       "        [5.1285],\n",
       "        [6.9075],\n",
       "        [7.0458],\n",
       "        [8.4333],\n",
       "        [7.7314],\n",
       "        [6.8266],\n",
       "        [7.2033],\n",
       "        [6.7984],\n",
       "        [7.3178],\n",
       "        [6.5508],\n",
       "        [7.2182],\n",
       "        [7.8842],\n",
       "        [6.4338],\n",
       "        [8.1656],\n",
       "        [7.5109],\n",
       "        [6.7918],\n",
       "        [6.8768],\n",
       "        [6.6847],\n",
       "        [8.0324],\n",
       "        [7.4298],\n",
       "        [7.0668],\n",
       "        [8.5157],\n",
       "        [7.7633],\n",
       "        [7.1457],\n",
       "        [7.7736]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_sum = torch.sum(exp_tensor, 1).view((64, -1))\n",
    "exp_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1208, 0.0697, 0.1208, 0.1206, 0.1166, 0.0473, 0.1201, 0.1208, 0.0445,\n",
       "         0.1188],\n",
       "        [0.1161, 0.1149, 0.1150, 0.1159, 0.0907, 0.0428, 0.0689, 0.1161, 0.1160,\n",
       "         0.1038],\n",
       "        [0.1281, 0.1001, 0.1122, 0.1281, 0.0471, 0.0471, 0.1280, 0.1281, 0.0580,\n",
       "         0.1231],\n",
       "        [0.1339, 0.0493, 0.1313, 0.1339, 0.0499, 0.0493, 0.1315, 0.1339, 0.1339,\n",
       "         0.0533],\n",
       "        [0.1495, 0.0554, 0.1494, 0.1495, 0.0552, 0.0550, 0.0554, 0.1495, 0.0828,\n",
       "         0.0982],\n",
       "        [0.1587, 0.0584, 0.1586, 0.1587, 0.0734, 0.0584, 0.0584, 0.1587, 0.0584,\n",
       "         0.0584],\n",
       "        [0.1399, 0.1351, 0.1393, 0.1010, 0.0515, 0.0515, 0.0515, 0.1399, 0.0516,\n",
       "         0.1389],\n",
       "        [0.1635, 0.0602, 0.1356, 0.1635, 0.0607, 0.0602, 0.0711, 0.1635, 0.0608,\n",
       "         0.0608],\n",
       "        [0.1120, 0.1063, 0.1120, 0.1120, 0.0412, 0.1107, 0.1120, 0.1120, 0.0699,\n",
       "         0.1119],\n",
       "        [0.1461, 0.1211, 0.1069, 0.0690, 0.0538, 0.0538, 0.1009, 0.1461, 0.0698,\n",
       "         0.1326],\n",
       "        [0.1555, 0.0581, 0.1544, 0.0844, 0.0795, 0.0573, 0.0580, 0.1555, 0.1097,\n",
       "         0.0877],\n",
       "        [0.1174, 0.0432, 0.1170, 0.1109, 0.1142, 0.0432, 0.1092, 0.1174, 0.1104,\n",
       "         0.1173],\n",
       "        [0.1386, 0.0512, 0.1369, 0.1379, 0.0511, 0.0510, 0.0576, 0.1386, 0.1379,\n",
       "         0.0991],\n",
       "        [0.1276, 0.0470, 0.1276, 0.1275, 0.0471, 0.0810, 0.1113, 0.1276, 0.0757,\n",
       "         0.1276],\n",
       "        [0.1265, 0.0465, 0.1265, 0.1265, 0.0466, 0.0465, 0.1153, 0.1265, 0.1125,\n",
       "         0.1265],\n",
       "        [0.1581, 0.0592, 0.0795, 0.1581, 0.0583, 0.0582, 0.0704, 0.1581, 0.0593,\n",
       "         0.1407],\n",
       "        [0.1510, 0.0557, 0.1499, 0.1104, 0.0557, 0.1445, 0.0556, 0.1510, 0.0560,\n",
       "         0.0700],\n",
       "        [0.1315, 0.0510, 0.1315, 0.1305, 0.1279, 0.0493, 0.0657, 0.1315, 0.1315,\n",
       "         0.0498],\n",
       "        [0.1241, 0.0771, 0.1396, 0.1416, 0.0521, 0.0521, 0.0521, 0.1416, 0.1267,\n",
       "         0.0931],\n",
       "        [0.1393, 0.1011, 0.1238, 0.1397, 0.0514, 0.0514, 0.0515, 0.1397, 0.1397,\n",
       "         0.0625],\n",
       "        [0.1350, 0.1232, 0.1349, 0.1350, 0.0497, 0.0497, 0.0531, 0.1350, 0.0572,\n",
       "         0.1274],\n",
       "        [0.1578, 0.0581, 0.1578, 0.1003, 0.0582, 0.0582, 0.0581, 0.1578, 0.0627,\n",
       "         0.1308],\n",
       "        [0.1527, 0.0562, 0.1527, 0.1527, 0.0593, 0.0644, 0.0699, 0.1527, 0.0832,\n",
       "         0.0562],\n",
       "        [0.1201, 0.0876, 0.1410, 0.1410, 0.1007, 0.0519, 0.0519, 0.1410, 0.0522,\n",
       "         0.1127],\n",
       "        [0.1735, 0.0638, 0.1102, 0.0668, 0.0796, 0.0638, 0.0645, 0.1735, 0.1403,\n",
       "         0.0639],\n",
       "        [0.1301, 0.0485, 0.1297, 0.1076, 0.0479, 0.0479, 0.1292, 0.1301, 0.1301,\n",
       "         0.0989],\n",
       "        [0.1334, 0.1329, 0.1337, 0.1337, 0.0492, 0.0492, 0.0492, 0.1337, 0.1336,\n",
       "         0.0513],\n",
       "        [0.1544, 0.0568, 0.0880, 0.1538, 0.0662, 0.0568, 0.0570, 0.1544, 0.0597,\n",
       "         0.1529],\n",
       "        [0.1595, 0.0587, 0.1595, 0.1595, 0.0588, 0.0587, 0.0606, 0.1595, 0.0665,\n",
       "         0.0587],\n",
       "        [0.1578, 0.0581, 0.1578, 0.1574, 0.0586, 0.0584, 0.0778, 0.1578, 0.0581,\n",
       "         0.0583],\n",
       "        [0.1535, 0.0565, 0.1482, 0.1529, 0.0705, 0.0565, 0.0717, 0.1535, 0.0604,\n",
       "         0.0762],\n",
       "        [0.1520, 0.0774, 0.1516, 0.1520, 0.0698, 0.0572, 0.0559, 0.1520, 0.0758,\n",
       "         0.0562],\n",
       "        [0.1265, 0.1139, 0.1265, 0.1265, 0.0777, 0.0466, 0.0597, 0.1265, 0.0789,\n",
       "         0.1171],\n",
       "        [0.1398, 0.0519, 0.1384, 0.1394, 0.0514, 0.0524, 0.0517, 0.1398, 0.1396,\n",
       "         0.0956],\n",
       "        [0.1248, 0.1143, 0.1243, 0.1244, 0.0459, 0.0459, 0.1204, 0.1248, 0.0503,\n",
       "         0.1248],\n",
       "        [0.1518, 0.0560, 0.1518, 0.1481, 0.0560, 0.0559, 0.0559, 0.1518, 0.1166,\n",
       "         0.0561],\n",
       "        [0.1563, 0.0577, 0.1563, 0.1563, 0.0662, 0.0587, 0.0575, 0.1563, 0.0579,\n",
       "         0.0767],\n",
       "        [0.1370, 0.1257, 0.1372, 0.1372, 0.0545, 0.0505, 0.0612, 0.1372, 0.0906,\n",
       "         0.0689],\n",
       "        [0.1200, 0.0554, 0.1197, 0.1200, 0.0659, 0.0441, 0.1152, 0.1200, 0.1196,\n",
       "         0.1200],\n",
       "        [0.1950, 0.0717, 0.0842, 0.0738, 0.0739, 0.0718, 0.0727, 0.1950, 0.0717,\n",
       "         0.0901],\n",
       "        [0.1448, 0.0543, 0.1430, 0.1402, 0.0533, 0.0533, 0.1382, 0.1448, 0.0615,\n",
       "         0.0667],\n",
       "        [0.1419, 0.0540, 0.1419, 0.1419, 0.0535, 0.0522, 0.1402, 0.1419, 0.0785,\n",
       "         0.0539],\n",
       "        [0.1186, 0.0464, 0.1185, 0.1181, 0.1035, 0.0436, 0.1170, 0.1186, 0.0999,\n",
       "         0.1158],\n",
       "        [0.1293, 0.0683, 0.1292, 0.1293, 0.0476, 0.0516, 0.1024, 0.1293, 0.1293,\n",
       "         0.0835],\n",
       "        [0.1465, 0.0540, 0.1465, 0.0564, 0.0541, 0.0539, 0.0550, 0.1465, 0.1438,\n",
       "         0.1434],\n",
       "        [0.1388, 0.1093, 0.1388, 0.1388, 0.0512, 0.0712, 0.0606, 0.1388, 0.0511,\n",
       "         0.1013],\n",
       "        [0.1471, 0.0541, 0.1471, 0.1471, 0.0589, 0.1333, 0.0542, 0.1471, 0.0541,\n",
       "         0.0570],\n",
       "        [0.1365, 0.0990, 0.1366, 0.1366, 0.0503, 0.0503, 0.0503, 0.1367, 0.1366,\n",
       "         0.0672],\n",
       "        [0.1526, 0.1080, 0.1124, 0.1463, 0.0848, 0.0562, 0.0637, 0.1527, 0.0564,\n",
       "         0.0669],\n",
       "        [0.1385, 0.0524, 0.1382, 0.1384, 0.0510, 0.0510, 0.1385, 0.1385, 0.0520,\n",
       "         0.1015],\n",
       "        [0.1268, 0.1267, 0.1267, 0.1268, 0.0467, 0.0468, 0.0510, 0.1268, 0.1143,\n",
       "         0.1075],\n",
       "        [0.1554, 0.0575, 0.0819, 0.1554, 0.0575, 0.0572, 0.0684, 0.1554, 0.0572,\n",
       "         0.1541],\n",
       "        [0.1225, 0.0455, 0.1218, 0.1225, 0.0611, 0.0461, 0.1204, 0.1225, 0.1157,\n",
       "         0.1220],\n",
       "        [0.1331, 0.1326, 0.1331, 0.1331, 0.0510, 0.0490, 0.0511, 0.1331, 0.1331,\n",
       "         0.0507],\n",
       "        [0.1472, 0.0551, 0.0925, 0.0909, 0.0623, 0.0555, 0.1472, 0.1472, 0.0549,\n",
       "         0.1472],\n",
       "        [0.1454, 0.0552, 0.1454, 0.1454, 0.0535, 0.0537, 0.1309, 0.1454, 0.0535,\n",
       "         0.0715],\n",
       "        [0.1496, 0.0555, 0.1495, 0.1496, 0.0555, 0.0550, 0.0558, 0.1496, 0.1227,\n",
       "         0.0573],\n",
       "        [0.1245, 0.0458, 0.1245, 0.1245, 0.1242, 0.0462, 0.1186, 0.1245, 0.1164,\n",
       "         0.0508],\n",
       "        [0.1346, 0.0560, 0.1346, 0.1346, 0.0498, 0.0495, 0.1138, 0.1346, 0.1072,\n",
       "         0.0854],\n",
       "        [0.1415, 0.0538, 0.1415, 0.1415, 0.0524, 0.0529, 0.0521, 0.1415, 0.1238,\n",
       "         0.0991],\n",
       "        [0.1174, 0.1132, 0.1072, 0.1173, 0.0432, 0.0432, 0.1174, 0.1174, 0.1111,\n",
       "         0.1125],\n",
       "        [0.1288, 0.0474, 0.1288, 0.1288, 0.1082, 0.0476, 0.1041, 0.1288, 0.0488,\n",
       "         0.1286],\n",
       "        [0.1399, 0.0669, 0.1040, 0.1399, 0.0516, 0.0518, 0.0667, 0.1399, 0.0994,\n",
       "         0.1399],\n",
       "        [0.1286, 0.1250, 0.1286, 0.1259, 0.0486, 0.0473, 0.0956, 0.1286, 0.1198,\n",
       "         0.0517]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = exp_tensor / exp_sum\n",
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(softmax[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the output adds uppto one here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(tensor_a):\n",
    "    exp_tensor = torch.exp(-h2)\n",
    "    exp_sum = torch.sum(exp_tensor, dim=1).view((64, -1))\n",
    "    return exp_tensor / exp_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = softmax(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(probabilities, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building network through pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784, 256, bias=True)\n",
    "        # inputs to output layer with 10 classes\n",
    "        self.output = nn.Linear(256, 10, bias=True)\n",
    "        \n",
    "        # define sigmoid activation and softmax output\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of the operation\n",
    "        \n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Network(nn.Module):\n",
    "```\n",
    "\n",
    "Here we're inheriting from nn.Module. Combined with super().__init__() this creates a class that tracks the architecture and provides a lot of useful methods and attributes. It is mandatory to inherit from nn.Module when you're creating a class for your network. The name of the class itself can be anything.\n",
    "\n",
    "```python\n",
    "self.hidden = nn.Linear(784, 256)\n",
    "```\n",
    "This line creates a module for a linear transformation,  𝑥𝐖+𝑏 , with 784 inputs and 256 outputs and assigns it to self.hidden. The module automatically creates the weight and bias tensors which we'll use in the forward method. You can access the weight and bias tensors once the network (net) is created with net.hidden.weight and net.hidden.bias.\n",
    "\n",
    "```python\n",
    "self.output = nn.Linear(256, 10)\n",
    "```\n",
    "Similarly, this creates another linear transformation with 256 inputs and 10 outputs.\n",
    "\n",
    "```python\n",
    "self.sigmoid = nn.Sigmoid()\n",
    "self.softmax = nn.Softmax(dim=1)\n",
    "```\n",
    "Here I defined operations for the sigmoid activation and softmax output. Setting dim=1 in nn.Softmax(dim=1) calculates softmax across the columns.\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "```\n",
    "PyTorch networks created with nn.Module must have a forward method defined. It takes in a tensor x and passes it through the operations you defined in the __init__ method.\n",
    "\n",
    "```python\n",
    "x = self.hidden(x)\n",
    "x = self.sigmoid(x)\n",
    "x = self.output(x)\n",
    "x = self.softmax(x)\n",
    "```\n",
    "Here the input tensor x is passed through each operation and reassigned to x. We can see that the input tensor goes through the hidden layer, then a sigmoid function, then the output layer, and finally the softmax function. It doesn't matter what you name the variables here, as long as the inputs and outputs of the operations match the network architecture you want to build. The order in which you define things in the __init__ method doesn't matter, but you'll need to sequence the operations correctly in the forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional module pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do these things more concisely and clearly using the `torch.nn.functional` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784, 256, bias=True)\n",
    "        # inputs to output layer with 10 classes\n",
    "        self.output = nn.Linear(256, 10, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of the operation\n",
    "        \n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use ReLu as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128, bias=True)\n",
    "        self.fc2 = nn.Linear(128, 64, bias=True)\n",
    "        self.fc3 = nn.Linear(64, 10, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0193, -0.0167,  0.0210,  ...,  0.0223,  0.0291, -0.0133],\n",
       "        [ 0.0215, -0.0284, -0.0357,  ..., -0.0227, -0.0167,  0.0341],\n",
       "        [ 0.0054, -0.0292, -0.0304,  ..., -0.0090,  0.0034,  0.0075],\n",
       "        ...,\n",
       "        [ 0.0131,  0.0345, -0.0220,  ...,  0.0344, -0.0295,  0.0275],\n",
       "        [ 0.0221, -0.0034, -0.0208,  ...,  0.0210, -0.0257, -0.0282],\n",
       "        [-0.0304,  0.0198,  0.0025,  ...,  0.0061, -0.0062, -0.0158]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0085,  0.0105, -0.0260,  0.0227, -0.0104,  0.0355,  0.0287,  0.0235,\n",
       "        -0.0169, -0.0057, -0.0033,  0.0316,  0.0087,  0.0241, -0.0096,  0.0144,\n",
       "        -0.0337,  0.0177, -0.0336, -0.0102,  0.0188,  0.0034, -0.0345, -0.0208,\n",
       "         0.0174, -0.0082,  0.0300, -0.0227,  0.0153, -0.0040,  0.0177, -0.0099,\n",
       "         0.0195,  0.0317,  0.0286,  0.0013, -0.0027, -0.0163,  0.0261, -0.0231,\n",
       "         0.0158,  0.0088, -0.0251, -0.0152, -0.0202, -0.0094,  0.0161,  0.0087,\n",
       "         0.0224, -0.0127,  0.0349, -0.0310, -0.0176,  0.0337,  0.0061, -0.0255,\n",
       "         0.0109, -0.0016, -0.0002,  0.0032,  0.0184,  0.0097,  0.0080,  0.0287,\n",
       "        -0.0197,  0.0194,  0.0093,  0.0311,  0.0080,  0.0122,  0.0185,  0.0014,\n",
       "        -0.0256,  0.0154,  0.0284, -0.0299,  0.0132,  0.0157, -0.0308,  0.0260,\n",
       "         0.0136,  0.0327, -0.0255, -0.0002, -0.0291, -0.0006, -0.0229,  0.0213,\n",
       "        -0.0340, -0.0214, -0.0004, -0.0284, -0.0153,  0.0156,  0.0002,  0.0051,\n",
       "        -0.0090, -0.0318,  0.0299, -0.0071, -0.0289,  0.0102, -0.0306, -0.0138,\n",
       "        -0.0353, -0.0310,  0.0118,  0.0328, -0.0137, -0.0157, -0.0171, -0.0270,\n",
       "         0.0091,  0.0058,  0.0126,  0.0314,  0.0276,  0.0264, -0.0096, -0.0225,\n",
       "         0.0215, -0.0077, -0.0202,  0.0297,  0.0077,  0.0164,  0.0119, -0.0084],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight\n",
    "model.fc1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0019,  0.0076,  0.0088,  ..., -0.0084,  0.0004,  0.0010],\n",
       "        [-0.0013, -0.0228, -0.0050,  ...,  0.0149,  0.0009, -0.0116],\n",
       "        [ 0.0063,  0.0160,  0.0050,  ...,  0.0132, -0.0130, -0.0019],\n",
       "        ...,\n",
       "        [-0.0005,  0.0052, -0.0152,  ..., -0.0044, -0.0094,  0.0101],\n",
       "        [ 0.0004, -0.0101,  0.0001,  ..., -0.0089,  0.0020, -0.0047],\n",
       "        [ 0.0048, -0.0181,  0.0127,  ..., -0.0025, -0.0072,  0.0082]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
